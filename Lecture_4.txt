Deep Generative Modeling

Supervised Learning: data and labels for that data are given.
Data (x,y) where x is the data and y is the label and our goal to to map a function 
from data to the labels.
ex: classification regression, object detection etc.

Unsuperivsed Learning: only data is given, no labels.
the goal is to learn some hidden or underlying sturcture of the data 
ex: clustering, feature reduction, etc.


Generative Modeling: 
Goal: take as input training samples from some distribution and learn a model that represents 
that distribution.
ways to conduct generative modeling:
Density Estimation -> learn the approximation of the probability distribution of the sample data
Sample Generation -> learn a model of the sample data distribution and generate samples that fall in line to the true data 
both of these generate a probability distribution similar to the original distribution of data.

Latent variables: underlying variables that govern distribution behavior that we cant directly observe.

Autoencoders is an unsupervised approach (form of compression) for learning a lower dimensional feature representation 
from unlabeled training data. We can use autoencoders to learn the latent space of the sample by training the model
to use the autoencoded features to reconstruct the original data.

Autoencoders for representation learning

Bottleneck hidden layer forces network to learn a compressed latent
representation

Reconstruction loss forces the latent representation to capture (or
encode) as much “information” about the data as possible

Autoencoding = Automatically encoding data; Auto” = self-encoding

Variational autoencoders are a probabilistic twist on autoencoders as tehy sample from the mean and standard 
deviation to compute the latent sample.

properties to achieve from regularization.
1. Continuity: points that are close in latent space => similar content after decoding
2. Completeness: sampling from latent space => “meaningful” content after decoding

VAE L(omega,theta,x) = reconstruction loss + regularization term

VAE summary
1. Compress representation of world to something we can use to learn
2. Reconstruction allows for unsupervised learning (no labels!)
3. Reparameterization trick to train end-to-end
4. Interpret hidden latent variables using perturbation

Generative Adversarial Networks (GANs) are a way to make a generative
model by having two neural networks compete with each other.




