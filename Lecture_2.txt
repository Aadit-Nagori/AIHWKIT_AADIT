Lecture 2 RNN's

perceptrons take input and produce the output.

Neurons with Recurrence.
we can create sequential output data by relating outputs generated by greater timesteps to inputs from earlier timestpes with a 
recurrence relation. this can be done by using an internal memory or state linking timesteps to each other.
y(t) = f(x(t),h(t-1)) -> y being the ouput, x being the input and h being the past memory with respect to time t.
 
RNN - recurrent neural network.
rnn maintains an internal stat which is updated at each timestep as the sequence is processed.
this is done by applying a recurrence relation at every step.
h(t) = f(w)(x(t),h(t-1) -> h being the cell state, with h(t-1) being the old state, x being the input and w being the weights which 
parameterizes the function f. w (weight matrices) stays constant across all timesteps. 

RNN from scratch code example:
class MyRNNCell(tf keras layers Layer):
   def _init_(self, rnn_units, input_dim, output_dim):
     super(MyRNNCe11, self) _init_()

     #Initialize weight matrices
     self W_xh - self add_weight([rnn_units, input_dim])
     self w_hh self add_weight(inn_units, rnn_units])
     self f_w_hy - self add_weight([output_dim, rnn_units))

     #Initialize hidden state to zeros
    self.h - tf zeros(rnn_units, 11)


   def call(self, x):
     #Update the hidden state
     self h - tf math tanh( self.w_hh*self.h + self f.w_xh*x )
                                                 
     #Compute the output                 
     output - self.w_hy + self.h

     #Return the current output and hidden state
     return output, self.h

Tensorflow RNN implementation: tf.keras.layers.SimpleRNN(rnn_units) -> SimpleRNN is the rnn layer implementation.

Types of RNNs for Sequence Modeling -> with examples:
1:1 Vanilla NN -> binary classification
Many:1 -> Sentiment Classification
1:Many -> Text Generation, Image Captioning  
Many:Many -> Translation and Forecasting, Music Generation

Sequence Modeling Design Criteria 
1. Handle sequences of varying lengths
2. Track long-term dependencies relate points from early on to later on in the sequence
3. Maintain and preserve information about order
4. Share parameters across the sequence
RNNS meet this criteria 

Predicting next work example
Neural networks cannot interpret words and require numerical inputs
Embedding: transform indexes int a vector of fixed size.
examples of embedding -> one hot embedding, relative embedding

Backpropagation through time:
between each timestep, matrix multiplication to compute local gradient at that timestep is required, hence 
to calculate the total gradient, matrix multiplication of many factors of weight matrix and gradient computation is required which 
could lead to problems.
Gradient problems:
many weight values greater than 1 causing the gradients to explode during training, causing the gradient values to become extremely large, 
preventing optimization. -> can be solved by clipping gradients to smaller values to scale the bigger gradients.
Many values lesser than one causing the gradients to vanish.
Gradient vanishing solutions:
1. Activation Function Choice -> ReLU function
2. Parameter Initialization -> initialise weights to identity matrix preventing weights to shrink to 0 too rapidly
3. Gated Cells -> gates selectively add or remove information within each recurrent unit. Ex. LSTM

LSTM key concepts:
I. Maintain a cell state
2. Use gates to control the flow of information
     Forget gate gets rid of irrelevant information
     Store relevant information from current input
     Selectively update cell state
     Output gate returns a filtered version of the cell state
3. Backpropagation through time with partially uninterrupted gradient flow i.e. makes it more stable

Limitations of RNNs:
1. Encoding bottlenecks -> loosing information in the process
2. no parallelization hence very slow and inefficient
3. Not long memory, low memory capacity



