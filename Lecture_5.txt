Deep Reinforcement Learning

Reinforcement learning: 
data is only given as state-action pairs where states are observations of the data
and action are the behaviours of the data.
the goal is the maximize teh future rewards over many time steps.


Key concepts:
Agent -> takes actions
environment -> the world in which the agent exists and operates
Action -> a move the agent can make in the environment
Action space -> the set of possible actions an agent can make in the environment.
observations -> seen changes of the environment after taking actions
state -> a situation which the agent perceives 
reward -> feedback that measures the success or failure of the agents action

total reward is the discounted sum of all rewards obtained from time t

Q(S, a) = E[R|s,a]

The Q-function captures the expected total future reward an
agent in state, s, can receive by executing a certain action, a
the agent needs a policy pi to inder the best action to take at its state,s 
the strategy is the choose an action that maximizes future reward
pi*(s) = argmax Q(s,a)

Deep reinforcement learning algorithms 
1. value learning -> find Q(s,a) from a = argmax Q(s,a)
2. policy learning -> find pi(s) form the sample a ~ pi(s)

the action is the input, the agent is the Deep Neural Network to which the input is feeded into and 
the output of the DNN is the Q function


Downsides of Q-learning

Complexity:
* Can model scenarios where the action space is discrete and small
* Cannot handle continuous action spaces

Flexibility:
* Policy is deterministically computed from the Q function by maximizing the
reward => cannot learn stochastic policies

To address these, consider a new class of RL training algorithms: Policy gradient methods

In policy learning we directly optimize the policy pi(s)
Policy gradient enables modeling of continuous action space.


Training Policy Gradients

Training Algorithm
1. Initialize the agent
2. Run a policy until termination
3, Record all states, actions, rewards
4. Decrease probability of actions that
resulted in low reward
5. Increase probability of actions that
resulted in high reward




